{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class ROIPool(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.max_pool = nn.AdaptiveMaxPool2d(7)\\n        \\n    def forward(self, inputs, rois):\\n        #rois: [torch(2000,4) x bs] \\n        #output: bs, 2000, ch, h, w\\n        #print(inputs.shape, len(rois))\\n        \\n        #n = min(list(map(lambda x: x.shape[0], rois)))\\n        #for i, tensor in enumerate(rois):\\n        #    rois[i] = rois[i][:n,:]\\n        #rois = torch.stack(rois, dim=0) #tensor (bs, n, 4)\\n        \\n        #print(rois.shape)\\n        bs, n = rois.shape[0], rois.shape[1]\\n        x1 = rois[:,:,0]\\n        x2 = rois[:,:,2]\\n        y1 = rois[:,:,1]\\n        y2 = rois[:,:,3]\\n        h, w = inputs.shape[2], inputs.shape[3]\\n        x1 = torch.floor(x1/512*w).type(torch.uint8)\\n        x2 = torch.ceil(x2/512*w).type(torch.uint8)\\n        y1 = torch.floor(y1/512*h).type(torch.uint8)\\n        y2 = torch.ceil(y2/512*h).type(torch.uint8)\\n        \\n        res = []\\n        for batch in range(bs):\\n            for i in range(n):\\n                inp = inputs[batch, :, y1[batch,i]:y2[batch,i], x1[batch,i]:x2[batch,i]].unsqueeze(0) # 1, ch, h',w'\\n                inp = self.max_pool(inp)\\n                res.append(inp)\\n        res = torch.cat(res, dim=0) #batch*dim, ch, 7, 7\\n        return res\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class ROIPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(7)\n",
    "        \n",
    "    def forward(self, inputs, rois):\n",
    "        #rois: [torch(2000,4) x bs] \n",
    "        #output: bs, 2000, ch, h, w\n",
    "        #print(inputs.shape, len(rois))\n",
    "        \n",
    "        #n = min(list(map(lambda x: x.shape[0], rois)))\n",
    "        #for i, tensor in enumerate(rois):\n",
    "        #    rois[i] = rois[i][:n,:]\n",
    "        #rois = torch.stack(rois, dim=0) #tensor (bs, n, 4)\n",
    "        \n",
    "        #print(rois.shape)\n",
    "        bs, n = rois.shape[0], rois.shape[1]\n",
    "        x1 = rois[:,:,0]\n",
    "        x2 = rois[:,:,2]\n",
    "        y1 = rois[:,:,1]\n",
    "        y2 = rois[:,:,3]\n",
    "        h, w = inputs.shape[2], inputs.shape[3]\n",
    "        x1 = torch.floor(x1/512*w).type(torch.uint8)\n",
    "        x2 = torch.ceil(x2/512*w).type(torch.uint8)\n",
    "        y1 = torch.floor(y1/512*h).type(torch.uint8)\n",
    "        y2 = torch.ceil(y2/512*h).type(torch.uint8)\n",
    "        \n",
    "        res = []\n",
    "        for batch in range(bs):\n",
    "            for i in range(n):\n",
    "                inp = inputs[batch, :, y1[batch,i]:y2[batch,i], x1[batch,i]:x2[batch,i]].unsqueeze(0) # 1, ch, h',w'\n",
    "                inp = self.max_pool(inp)\n",
    "                res.append(inp)\n",
    "        res = torch.cat(res, dim=0) #batch*dim, ch, 7, 7\n",
    "        return res\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import bbox_collate\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from utils import data2target\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.ops.boxes import box_iou\n",
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "from make_dloader import make_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('./config.yaml'))\n",
    "dataset_means = json.load(open(config['dataset']['mean_file']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ROIPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, inputs, rois):\n",
    "        #rois: bs,2000,4 \n",
    "        #output: bs, 2000, ch, h, w\n",
    "        rois = [r for r in rois]\n",
    "        h, w = inputs.shape[2], inputs.shape[3]\n",
    "        res = roi_align(inputs, rois, 7, spatial_scale=w/512)\n",
    "        return res\n",
    "    \n",
    "class GAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.att = nn.Sequential(nn.Conv2d(2048, 2048, 1),\n",
    "                                                  nn.BatchNorm2d(2048),\n",
    "                                                  nn.Sigmoid())\n",
    "        self.conv_7 = nn.Conv2d(2048, 3, 1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, inputs, label):\n",
    "        attention = self.att(inputs)\n",
    "        f = self.conv_7(attention)\n",
    "        f = self.gap(f).squeeze()\n",
    "        loss = self.loss(f,label)\n",
    "        return attention, loss\n",
    "        \n",
    "\n",
    "class vector_extractor_gam(nn.Module):\n",
    "    \"\"\"input: images, proposals\n",
    "         output: feature vector\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        layers = list(model.children())[:-2]\n",
    "        self.feature_map = nn.Sequential(*layers)\n",
    "        self.gam = GAM()\n",
    "        self.roi_pool = _ROIPool()\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feature_vector = nn.Sequential(nn.Linear(2048, 1000),\n",
    "                                            nn.ReLU(inplace=True),\n",
    "                                            nn.Linear(1000, 500),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "    def forward(self, inputs, labels, rois):\n",
    "        f = self.feature_map(inputs)\n",
    "        attention, loss = self.gam(f, labels)\n",
    "        f = (1+attention)*f\n",
    "        f = self.roi_pool(f, rois)\n",
    "        f = self.gap(f).view(f.shape[0], f.shape[1]) #batch*proposal, ch\n",
    "        f = self.feature_vector(f) \n",
    "        return f, loss\n",
    "    \n",
    "class MIDN(nn.Module):\n",
    "    \"\"\"input: feature vector, labels\n",
    "         output: scores per proposal, loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c_in = 500\n",
    "        self.layer_c = nn.Linear(c_in, 3)\n",
    "        self.layer_d = nn.Linear(c_in, 3)\n",
    "        self.softmax_c = nn.Softmax(dim=2)\n",
    "        self.softmax_d = nn.Softmax(dim=1)\n",
    "        self.loss = None\n",
    "    def forward(self, inputs, labels, num):\n",
    "        bs, proposal = inputs.shape[0]//num, num\n",
    "        x_c = self.layer_c(inputs).view(bs, proposal, -1) #bs, proposal, 3\n",
    "        x_d = self.layer_d(inputs).view(bs, proposal, -1)\n",
    "        sigma_c = self.softmax_c(x_c)\n",
    "        sigma_d = self.softmax_d(x_d)\n",
    "        x_r = sigma_c * sigma_d #bs, proposal, 3\n",
    "        if not self.training:\n",
    "            return x_r\n",
    "        phi_c = x_r.sum(dim=1) #bs, 3\n",
    "        #if not self.training:\n",
    "        #    return phi_c\n",
    "        #print(phi_c)\n",
    "        #phi_c = logit(phi_c)\n",
    "        #print(torch.isnan(phi_c))\n",
    "        if self.loss == None:\n",
    "            #pos_weight = torch.tensor([9.,16.0,16.]*bs).reshape(-1,3)\n",
    "            #self.loss = nn.BCEWithLogitsLoss(pos_weight = pos_weight.cuda())\n",
    "            self.loss = nn.BCELoss()\n",
    "        loss = self.loss(phi_c, labels)\n",
    "        \n",
    "        return x_r, loss\n",
    "        \n",
    "        \n",
    "class ICR(nn.Module):\n",
    "    \"\"\"input: feature vector (bs*proposal, ch)\n",
    "                     k-1th proposal scores(bs, proposal,3or4)\n",
    "                     supervision (label) (bs)\n",
    "                     ROI proposals\n",
    "         output: refined proposal scores, loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c_in = 500\n",
    "        self.I_t = 0.5\n",
    "        self.fc = nn.Linear(c_in, 4)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        #weight = torch.tensor([9.,16.0,13.,1.]).reshape(-1,4)\n",
    "        #self.loss = nn.CrossEntropyLoss(weight = weight, reduction=\"none\")\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        \"\"\"self.y_k = torch.zeros(bs, proposal, 4).cuda()\n",
    "        self.y_k[:, :, 3] = 1\n",
    "        self.w = torch.zeros(bs, proposal).cuda()\"\"\"\n",
    "        \n",
    "    def forward(self, inputs, pre_score, labels, rois, num):\n",
    "        bs, proposal = inputs.shape[0]//num, num\n",
    "        xr_k = self.fc(inputs).view(bs, proposal, -1) #bs, proposal, 4\n",
    "        xr_k = self.softmax(xr_k)\n",
    "        if not self.training:\n",
    "            return xr_k\n",
    "        _xr_k = xr_k.view(bs*proposal, -1)\n",
    "        self.y_k = torch.zeros(bs, proposal, 4).cuda()\n",
    "        self.y_k[:, :, 3] = 1\n",
    "        self.w = torch.zeros(bs, proposal).cuda()\n",
    "        I = torch.zeros(bs, proposal)\n",
    "        for batch in range(bs):\n",
    "            for c in range(3):\n",
    "                if labels[batch][c]:\n",
    "                    m = torch.max(pre_score[batch, :, c], 0)\n",
    "                    x = m[0].item()\n",
    "                    j = m[1].item()\n",
    "                    mat = box_iou(rois[batch], rois[batch][j].unsqueeze(0))\n",
    "                    for r in range(proposal):\n",
    "                        _I = mat[r][0]\n",
    "                        if _I > I[batch, r]:\n",
    "                            I[batch, r] = _I\n",
    "                            self.w[batch, r] = x\n",
    "                            if _I > self.I_t:\n",
    "                                self.y_k[batch, r, c] = 1\n",
    "                                self.y_k[batch, r, 3] = 0\n",
    "        self.y_k = self.y_k.view(bs*proposal, -1)\n",
    "        self.w = self.w.view(bs*proposal, 1)\n",
    "        loss = self.loss(_xr_k.cuda().float(), torch.max(self.y_k, 1)[1])\n",
    "        loss = torch.mean(self.w*loss)\n",
    "        return xr_k, loss\n",
    "\n",
    "class GAM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.v_extractor = vector_extractor_gam()\n",
    "        self.midn = MIDN()\n",
    "        self.icr1 = ICR()\n",
    "        self.icr2 = ICR()\n",
    "        self.icr3 = ICR()\n",
    "    \n",
    "    def forward(self, inputs, labels, rois, num):\n",
    "        if self.training:\n",
    "            labels = labels.squeeze()\n",
    "            rois = rois.squeeze()\n",
    "            v, loss_cl = self.v_extractor(inputs, labels, rois)\n",
    "            x, midn_loss = self.midn(v, labels, num)\n",
    "            x, loss1 = self.icr1(v, x, labels, rois, num)\n",
    "            x, loss2 = self.icr2(v, x, labels, rois, num)\n",
    "            x, loss3 = self.icr3(v, x, labels, rois, num) \n",
    "            loss = midn_loss + loss_cl + loss1 + loss2 + loss3\n",
    "            return x, loss  \n",
    "        else:\n",
    "            #rois = rois.squeeze()\n",
    "            v, _ = self.v_extractor(inputs, labels, rois)\n",
    "            x = self.midn(v, labels, num)\n",
    "            #x = self.icr3(v, None, None, rois, num) \n",
    "            x, rois = x[0], rois[0].cuda()\n",
    "            s, i = torch.max(x, 1)\n",
    "            sort = torch.argsort(s, descending=True)\n",
    "            s, i = s.view(-1,1), i.view(-1,1).cuda().float()\n",
    "            #print(s.shape, i.shape, rois.shape)\n",
    "            cat = torch.cat([s, i ,rois], dim=1)\n",
    "            cat = cat[sort, :]\n",
    "            scores = cat[:, 0]\n",
    "            labels = cat[:, 1]\n",
    "            bboxes = cat[:, 2:]\n",
    "            return scores, labels, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-748a43ad65fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mASPP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m  \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def  __init__(self,size_list=[]):\n",
    "        super().__init__()\n",
    "        assert len(size_list)>0\n",
    "        self.avgpool_list = []\n",
    "        for size in size_list:\n",
    "            self.avgpool_list.append(nn.Sequential(nn.AdaptiveAvgPool2d(size),\n",
    "                                                                                 nn.Flatten()))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        vec = []\n",
    "        for avgpool in self.avgpool_list:\n",
    "            vec.append(avgpool(x))\n",
    "        vec = torch.cat(vec,1)\n",
    "        return vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.50s)\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dl_t, dl_v, _, _, _ = make_data(batchsize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = GAM()\n",
    "#oicr = nn.DataParallel(oicr)\n",
    "gam.cuda()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(gam.parameters(), lr = 1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000, 1.253701090812683\r"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dl_t):\n",
    "    opt.zero_grad()\n",
    "    labels, n, t, v, u= data2target(data)\n",
    "    labels = labels.unsqueeze(1).unsqueeze(2).cuda().float() # bs, 1, 1, num_class\n",
    "    rois = [r.cuda().float() for r in data[\"p_bboxes\"]]\n",
    "    n = min(list(map(lambda x: x.shape[0], rois)))\n",
    "    for ind, tensor in enumerate(rois):\n",
    "        rois[ind] = rois[ind][:n,:]\n",
    "    rois = torch.stack(rois, dim=0) \n",
    "    rois = rois.unsqueeze(1) #bs, 1, n, 4\n",
    "    #print(data[\"img\"].shape, labels.unsqueeze(1).unsqueeze(2).shape, rois.shape)\n",
    "    output, loss = gam(data[\"img\"].cuda().float(), labels, rois, n)\n",
    "    loss = torch.mean(loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(f'{i}/{len(dl_t)}, {loss}', end='\\r')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
